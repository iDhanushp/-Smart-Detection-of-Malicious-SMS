"""
Multilingual SMS Fraud Detection Training
Supports multiple languages with language-specific models and detection
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import json
import os
from typing import List, Dict, Tuple, Optional
import re
from langdetect import detect, DetectorFactory
import pickle

# Set seed for consistent language detection
DetectorFactory.seed = 0

class MultilingualFraudDetector:
    """
    Multilingual SMS fraud detection system
    """
    
    def __init__(self):
        self.languages = ['en', 'hi', 'es', 'fr', 'de', 'zh', 'ar', 'ja', 'ko', 'ru']
        self.models = {}
        self.vectorizers = {}
        self.language_names = {
            'en': 'English',
            'hi': 'Hindi',
            'es': 'Spanish',
            'fr': 'French',
            'de': 'German',
            'zh': 'Chinese',
            'ar': 'Arabic',
            'ja': 'Japanese',
            'ko': 'Korean',
            'ru': 'Russian'
        }
        
    def detect_language(self, text: str) -> str:
        """
        Detect the language of the SMS text
        """
        try:
            # Clean text for better detection
            cleaned_text = self._clean_text_for_detection(text)
            
            if len(cleaned_text) < 10:
                return 'en'  # Default to English for very short texts
                
            detected_lang = detect(cleaned_text)
            
            # Map similar languages
            if detected_lang in ['zh-cn', 'zh-tw']:
                detected_lang = 'zh'
            elif detected_lang in ['ja', 'jp']:
                detected_lang = 'ja'
            elif detected_lang in ['ko', 'kr']:
                detected_lang = 'ko'
                
            # Return detected language if supported, otherwise default to English
            return detected_lang if detected_lang in self.languages else 'en'
            
        except Exception as e:
            print(f"Language detection failed: {e}")
            return 'en'  # Default to English
    
    def _clean_text_for_detection(self, text: str) -> str:
        """
        Clean text for language detection
        """
        # Remove URLs, numbers, and special characters
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def prepare_multilingual_data(self, data_path: str) -> Dict[str, pd.DataFrame]:
        """
        Prepare training data for multiple languages
        """
        print("Preparing multilingual training data...")
        
        # Load the main dataset
        df = pd.read_csv(data_path)
        
        # Language-specific datasets
        language_data = {}
        
        for lang in self.languages:
            print(f"Processing {self.language_names[lang]} data...")
            
            if lang == 'en':
                # Use existing English data
                language_data[lang] = df.copy()
            else:
                # For other languages, we would need language-specific datasets
                # For now, create synthetic data or use translation
                language_data[lang] = self._create_synthetic_data(df, lang)
        
        return language_data
    
    def _create_synthetic_data(self, base_df: pd.DataFrame, language: str) -> pd.DataFrame:
        """
        Create synthetic data for languages without specific datasets
        This is a placeholder - in production, you'd use real translated data
        """
        # Create a smaller synthetic dataset for demonstration
        synthetic_data = []
        
        # Fraudulent messages in different languages
        fraud_messages = {
            'hi': [
                'à¤†à¤ªà¤•à¤¾ à¤¬à¥ˆà¤‚à¤• à¤–à¤¾à¤¤à¤¾ à¤¨à¤¿à¤²à¤‚à¤¬à¤¿à¤¤ à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤ªà¥à¤¨à¤°à¥à¤¸à¤•à¥à¤°à¤¿à¤¯à¤£ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤²à¤¿à¤• à¤•à¤°à¥‡à¤‚',
                'à¤†à¤ªà¤¨à¥‡ 50000 à¤°à¥à¤ªà¤¯à¥‡ à¤œà¥€à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤…à¤­à¥€ à¤•à¥à¤²à¥‡à¤® à¤•à¤°à¥‡à¤‚',
                'à¤†à¤ªà¤•à¤¾ Aadhaar à¤¨à¤‚à¤¬à¤° à¤¬à¥à¤²à¥‰à¤• à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤¤à¥à¤°à¤‚à¤¤ à¤µà¥‡à¤°à¤¿à¤«à¤¾à¤ˆ à¤•à¤°à¥‡à¤‚',
                'à¤†à¤ªà¤•à¤¾ à¤ªà¥ˆà¤¨ à¤•à¤¾à¤°à¥à¤¡ à¤à¤•à¥à¤¸à¤ªà¤¾à¤¯à¤° à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤…à¤ªà¤¡à¥‡à¤Ÿ à¤•à¤°à¥‡à¤‚',
            ],
            'es': [
                'Su cuenta bancaria ha sido suspendida. Haga clic para reactivar',
                'Ha ganado $1000. Reclame ahora',
                'Su tarjeta de crÃ©dito ha sido bloqueada. Verifique inmediatamente',
                'Su cuenta necesita verificaciÃ³n urgente',
            ],
            'fr': [
                'Votre compte bancaire a Ã©tÃ© suspendu. Cliquez pour rÃ©activer',
                'Vous avez gagnÃ© 1000â‚¬. RÃ©clamez maintenant',
                'Votre carte de crÃ©dit a Ã©tÃ© bloquÃ©e. VÃ©rifiez immÃ©diatement',
                'Votre compte nÃ©cessite une vÃ©rification urgente',
            ],
            'de': [
                'Ihr Bankkonto wurde gesperrt. Klicken Sie zur Reaktivierung',
                'Sie haben 1000â‚¬ gewonnen. Jetzt einfordern',
                'Ihre Kreditkarte wurde gesperrt. Sofort Ã¼berprÃ¼fen',
                'Ihr Konto benÃ¶tigt dringende ÃœberprÃ¼fung',
            ],
            'zh': [
                'æ‚¨çš„é“¶è¡Œè´¦æˆ·å·²è¢«æš‚åœã€‚ç‚¹å‡»é‡æ–°æ¿€æ´»',
                'æ‚¨èµ¢å¾—äº†1000å…ƒã€‚ç«‹å³é¢†å–',
                'æ‚¨çš„ä¿¡ç”¨å¡å·²è¢«å†»ç»“ã€‚ç«‹å³éªŒè¯',
                'æ‚¨çš„è´¦æˆ·éœ€è¦ç´§æ€¥éªŒè¯',
            ],
            'ar': [
                'ØªÙ… ØªØ¹Ù„ÙŠÙ‚ Ø­Ø³Ø§Ø¨Ùƒ Ø§Ù„Ù…ØµØ±ÙÙŠ. Ø§Ù†Ù‚Ø± Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙØ¹ÙŠÙ„',
                'Ù„Ù‚Ø¯ Ø±Ø¨Ø­Øª 1000 Ø¯ÙˆÙ„Ø§Ø±. Ø§Ø·Ù„Ø¨ Ø§Ù„Ø¢Ù†',
                'ØªÙ… Ø­Ø¸Ø± Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. ØªØ­Ù‚Ù‚ ÙÙˆØ±Ø§Ù‹',
                'Ø­Ø³Ø§Ø¨Ùƒ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ù‚Ù‚ Ø¹Ø§Ø¬Ù„',
            ],
            'ja': [
                'ã‚ãªãŸã®éŠ€è¡Œå£åº§ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸã€‚å†é–‹ã™ã‚‹ã«ã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„',
                '1000å††ã‚’ç²å¾—ã—ã¾ã—ãŸã€‚ä»Šã™ãè«‹æ±‚ã—ã¦ãã ã•ã„',
                'ã‚ãªãŸã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚ã™ãã«ç¢ºèªã—ã¦ãã ã•ã„',
                'ã‚ãªãŸã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ç·Šæ€¥ç¢ºèªãŒå¿…è¦ã§ã™',
            ],
            'ko': [
                'ê·€í•˜ì˜ ì€í–‰ ê³„ì¢Œê°€ ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ê°œí•˜ë ¤ë©´ í´ë¦­í•˜ì„¸ìš”',
                '1000ì›ì„ íšë“í–ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆ ì²­êµ¬í•˜ì„¸ìš”',
                'ê·€í•˜ì˜ ì‹ ìš©ì¹´ë“œê°€ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸í•˜ì„¸ìš”',
                'ê·€í•˜ì˜ ê³„ì •ì€ ê¸´ê¸‰ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤',
            ],
            'ru': [
                'Ğ’Ğ°Ñˆ Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ',
                'Ğ’Ñ‹ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ğ°Ğ»Ğ¸ 1000 Ñ€ÑƒĞ±Ğ»ĞµĞ¹. Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚Ğµ ÑĞµĞ¹Ñ‡Ğ°Ñ',
                'Ğ’Ğ°ÑˆĞ° ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ·Ğ°Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾',
                'Ğ’Ğ°Ñˆ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸',
            ]
        }
        
        # Spam messages in different languages
        spam_messages = {
            'hi': [
                '50% à¤›à¥‚à¤Ÿ à¤ªà¤° à¤¸à¤­à¥€ à¤‰à¤¤à¥à¤ªà¤¾à¤¦à¥¤ à¤…à¤­à¥€ à¤–à¤°à¥€à¤¦à¥‡à¤‚',
                'à¤¨à¤¯à¤¾ à¤®à¥‹à¤¬à¤¾à¤‡à¤² à¤«à¥‹à¤¨ à¤®à¥à¤«à¥à¤¤ à¤®à¥‡à¤‚à¥¤ à¤…à¤­à¥€ à¤•à¥à¤²à¥‡à¤® à¤•à¤°à¥‡à¤‚',
                'à¤†à¤ªà¤•à¤¾ à¤²à¥‰à¤Ÿà¤°à¥€ à¤Ÿà¤¿à¤•à¤Ÿ à¤œà¥€à¤¤ à¤—à¤¯à¤¾ à¤¹à¥ˆ',
                'à¤¸à¤­à¥€ à¤¬à¥à¤°à¤¾à¤‚à¤¡à¥à¤¸ à¤ªà¤° à¤¬à¤¡à¤¼à¥€ à¤›à¥‚à¤Ÿ',
            ],
            'es': [
                '50% de descuento en todos los productos. Compre ahora',
                'TelÃ©fono mÃ³vil gratis. Reclame ahora',
                'Su boleto de loterÃ­a ha ganado',
                'Grandes descuentos en todas las marcas',
            ],
            'fr': [
                '50% de rÃ©duction sur tous les produits. Achetez maintenant',
                'TÃ©lÃ©phone portable gratuit. RÃ©clamez maintenant',
                'Votre billet de loterie a gagnÃ©',
                'Grosses rÃ©ductions sur toutes les marques',
            ],
            'de': [
                '50% Rabatt auf alle Produkte. Jetzt kaufen',
                'Kostenloses Handy. Jetzt einfordern',
                'Ihr Lottoschein hat gewonnen',
                'GroÃŸe Rabatte auf alle Marken',
            ],
            'zh': [
                'æ‰€æœ‰äº§å“50%æŠ˜æ‰£ã€‚ç«‹å³è´­ä¹°',
                'å…è´¹æ‰‹æœºã€‚ç«‹å³é¢†å–',
                'æ‚¨çš„å½©ç¥¨ä¸­å¥–äº†',
                'æ‰€æœ‰å“ç‰Œå¤§å‡ä»·',
            ],
            'ar': [
                'Ø®ØµÙ… 50% Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª. Ø§Ø´ØªØ± Ø§Ù„Ø¢Ù†',
                'Ù‡Ø§ØªÙ Ù…Ø­Ù…ÙˆÙ„ Ù…Ø¬Ø§Ù†ÙŠ. Ø§Ø·Ù„Ø¨ Ø§Ù„Ø¢Ù†',
                'ØªØ°ÙƒØ±Ø© Ø§Ù„ÙŠØ§Ù†ØµÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙØ§Ø²Øª',
                'Ø®ØµÙˆÙ…Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©',
            ],
            'ja': [
                'å…¨å•†å“50%ã‚ªãƒ•ã€‚ä»Šã™ãè³¼å…¥',
                'ç„¡æ–™æºå¸¯é›»è©±ã€‚ä»Šã™ãè«‹æ±‚',
                'ã‚ãªãŸã®å®ãã˜ãŒå½“é¸ã—ã¾ã—ãŸ',
                'å…¨ãƒ–ãƒ©ãƒ³ãƒ‰å¤§ã‚»ãƒ¼ãƒ«',
            ],
            'ko': [
                'ëª¨ë“  ì œí’ˆ 50% í• ì¸. ì§€ê¸ˆ êµ¬ë§¤í•˜ì„¸ìš”',
                'ë¬´ë£Œ íœ´ëŒ€í°. ì§€ê¸ˆ ì²­êµ¬í•˜ì„¸ìš”',
                'ê·€í•˜ì˜ ë³µê¶Œì´ ë‹¹ì²¨ë˜ì—ˆìŠµë‹ˆë‹¤',
                'ëª¨ë“  ë¸Œëœë“œ ëŒ€í­ í• ì¸',
            ],
            'ru': [
                '50% ÑĞºĞ¸Ğ´ĞºĞ° Ğ½Ğ° Ğ²ÑĞµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹. ĞŸĞ¾ĞºÑƒĞ¿Ğ°Ğ¹Ñ‚Ğµ ÑĞµĞ¹Ñ‡Ğ°Ñ',
                'Ğ‘ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½. Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚Ğµ ÑĞµĞ¹Ñ‡Ğ°Ñ',
                'Ğ’Ğ°Ñˆ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ»ĞµÑ‚ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ğ°Ğ»',
                'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞºĞ¸Ğ´ĞºĞ¸ Ğ½Ğ° Ğ²ÑĞµ Ğ±Ñ€ĞµĞ½Ğ´Ñ‹',
            ]
        }
        
        # Legitimate messages in different languages
        legitimate_messages = {
            'hi': [
                'à¤†à¤ªà¤•à¤¾ à¤‘à¤°à¥à¤¡à¤° à¤¡à¤¿à¤²à¥€à¤µà¤° à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆ',
                'à¤†à¤ªà¤•à¤¾ à¤¬à¥ˆà¤‚à¤• à¤²à¥‡à¤¨à¤¦à¥‡à¤¨ à¤¸à¤«à¤² à¤°à¤¹à¤¾',
                'à¤†à¤ªà¤•à¤¾ OTP 123456 à¤¹à¥ˆ',
                'à¤†à¤ªà¤•à¤¾ à¤°à¤¿à¤œà¤°à¥à¤µà¥‡à¤¶à¤¨ à¤•à¤¨à¥à¤«à¤°à¥à¤® à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆ',
            ],
            'es': [
                'Su pedido ha sido entregado',
                'Su transacciÃ³n bancaria fue exitosa',
                'Su OTP es 123456',
                'Su reserva ha sido confirmada',
            ],
            'fr': [
                'Votre commande a Ã©tÃ© livrÃ©e',
                'Votre transaction bancaire a rÃ©ussi',
                'Votre OTP est 123456',
                'Votre rÃ©servation a Ã©tÃ© confirmÃ©e',
            ],
            'de': [
                'Ihre Bestellung wurde geliefert',
                'Ihre Banktransaktion war erfolgreich',
                'Ihr OTP ist 123456',
                'Ihre Reservierung wurde bestÃ¤tigt',
            ],
            'zh': [
                'æ‚¨çš„è®¢å•å·²é€è¾¾',
                'æ‚¨çš„é“¶è¡Œäº¤æ˜“æˆåŠŸ',
                'æ‚¨çš„éªŒè¯ç æ˜¯123456',
                'æ‚¨çš„é¢„è®¢å·²ç¡®è®¤',
            ],
            'ar': [
                'ØªÙ… ØªØ³Ù„ÙŠÙ… Ø·Ù„Ø¨Ùƒ',
                'Ù…Ø¹Ø§Ù…Ù„ØªÙƒ Ø§Ù„Ù…ØµØ±ÙÙŠØ© Ù†Ø§Ø¬Ø­Ø©',
                'Ø±Ù…Ø² Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ùˆ 123456',
                'ØªÙ… ØªØ£ÙƒÙŠØ¯ Ø­Ø¬Ø²Ùƒ',
            ],
            'ja': [
                'ã”æ³¨æ–‡ãŒé…é”ã•ã‚Œã¾ã—ãŸ',
                'ãŠå®¢æ§˜ã®éŠ€è¡Œå–å¼•ãŒæˆåŠŸã—ã¾ã—ãŸ',
                'ã‚ãªãŸã®OTPã¯123456ã§ã™',
                'ã”äºˆç´„ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ',
            ],
            'ko': [
                'ì£¼ë¬¸ì´ ë°°ë‹¬ë˜ì—ˆìŠµë‹ˆë‹¤',
                'ê·€í•˜ì˜ ì€í–‰ ê±°ë˜ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤',
                'ê·€í•˜ì˜ OTPëŠ” 123456ì…ë‹ˆë‹¤',
                'ì˜ˆì•½ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤',
            ],
            'ru': [
                'Ğ’Ğ°Ñˆ Ğ·Ğ°ĞºĞ°Ğ· Ğ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½',
                'Ğ’Ğ°ÑˆĞ° Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ°Ñ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾',
                'Ğ’Ğ°Ñˆ OTP 123456',
                'Ğ’Ğ°ÑˆĞµ Ğ±Ñ€Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾',
            ]
        }
        
        # Create synthetic dataset
        for msg_type, messages in [('fraudulent', fraud_messages.get(language, [])),
                                  ('spam', spam_messages.get(language, [])),
                                  ('legitimate', legitimate_messages.get(language, []))]:
            for message in messages:
                synthetic_data.append({
                    'message': message,
                    'label': msg_type,
                    'language': language
                })
        
        return pd.DataFrame(synthetic_data)
    
    def train_multilingual_models(self, language_data: Dict[str, pd.DataFrame]):
        """
        Train language-specific models
        """
        print("Training multilingual models...")
        
        for lang, df in language_data.items():
            if df.empty:
                print(f"Skipping {self.language_names[lang]} - no data available")
                continue
                
            print(f"Training {self.language_names[lang]} model...")
            
            # Prepare features and labels
            X = df['message'].values
            y = df['label'].values
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Create TF-IDF vectorizer
            vectorizer = TfidfVectorizer(
                max_features=3000,
                ngram_range=(1, 2),
                stop_words=None,  # Language-specific stop words would be better
                min_df=2,
                max_df=0.95
            )
            
            # Fit and transform training data
            X_train_tfidf = vectorizer.fit_transform(X_train)
            X_test_tfidf = vectorizer.transform(X_test)
            
            # Train model
            model = MultinomialNB(alpha=1.0)
            model.fit(X_train_tfidf, y_train)
            
            # Evaluate model
            y_pred = model.predict(X_test_tfidf)
            accuracy = accuracy_score(y_test, y_pred)
            
            print(f"{self.language_names[lang]} model accuracy: {accuracy:.3f}")
            print(classification_report(y_test, y_pred))
            
            # Store model and vectorizer
            self.models[lang] = model
            self.vectorizers[lang] = vectorizer
    
    def classify_message(self, text: str) -> Dict[str, any]:
        """
        Classify message using appropriate language model
        """
        # Detect language
        detected_lang = self.detect_language(text)
        
        # Use detected language model if available, otherwise fallback to English
        if detected_lang in self.models:
            lang = detected_lang
        else:
            lang = 'en'
            print(f"Language {detected_lang} not supported, using English model")
        
        # Get model and vectorizer
        model = self.models[lang]
        vectorizer = self.vectorizers[lang]
        
        # Preprocess and predict
        text_vectorized = vectorizer.transform([text])
        prediction = model.predict(text_vectorized)[0]
        probabilities = model.predict_proba(text_vectorized)[0]
        
        # Get class labels
        classes = model.classes_
        
        # Create result
        result = {
            'prediction': prediction,
            'language': detected_lang,
            'language_name': self.language_names.get(detected_lang, 'Unknown'),
            'model_used': lang,
            'probabilities': dict(zip(classes, probabilities)),
            'confidence': max(probabilities)
        }
        
        return result
    
    def save_models(self, output_dir: str = "multilingual_models"):
        """
        Save all trained models and vectorizers
        """
        os.makedirs(output_dir, exist_ok=True)
        
        for lang in self.models.keys():
            # Save model
            model_path = os.path.join(output_dir, f"model_{lang}.pkl")
            with open(model_path, 'wb') as f:
                pickle.dump(self.models[lang], f)
            
            # Save vectorizer
            vectorizer_path = os.path.join(output_dir, f"vectorizer_{lang}.pkl")
            with open(vectorizer_path, 'wb') as f:
                pickle.dump(self.vectorizers[lang], f)
            
            # Save vocabulary
            vocab_path = os.path.join(output_dir, f"vocab_{lang}.json")
            vocabulary = self.vectorizers[lang].vocabulary_
            with open(vocab_path, 'w', encoding='utf-8') as f:
                json.dump(vocabulary, f, ensure_ascii=False, indent=2)
        
        # Save language mapping
        lang_map_path = os.path.join(output_dir, "language_mapping.json")
        with open(lang_map_path, 'w', encoding='utf-8') as f:
            json.dump(self.language_names, f, ensure_ascii=False, indent=2)
        
        print(f"Models saved to {output_dir}")
    
    def load_models(self, models_dir: str = "multilingual_models"):
        """
        Load trained models and vectorizers
        """
        if not os.path.exists(models_dir):
            print(f"Models directory {models_dir} not found")
            return
        
        # Load language mapping
        lang_map_path = os.path.join(models_dir, "language_mapping.json")
        if os.path.exists(lang_map_path):
            with open(lang_map_path, 'r', encoding='utf-8') as f:
                self.language_names = json.load(f)
        
        # Load models and vectorizers
        for lang in self.language_names.keys():
            model_path = os.path.join(models_dir, f"model_{lang}.pkl")
            vectorizer_path = os.path.join(models_dir, f"vectorizer_{lang}.pkl")
            
            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                with open(model_path, 'rb') as f:
                    self.models[lang] = pickle.load(f)
                
                with open(vectorizer_path, 'rb') as f:
                    self.vectorizers[lang] = pickle.load(f)
                
                print(f"Loaded {self.language_names[lang]} model")
        
        print(f"Loaded {len(self.models)} models")

def main():
    """
    Main function to demonstrate multilingual training
    """
    print("ğŸŒ Multilingual SMS Fraud Detection Training")
    
    # Initialize detector
    detector = MultilingualFraudDetector()
    
    # Test language detection
    test_messages = [
        "Your account has been suspended. Click here to verify",
        "à¤†à¤ªà¤•à¤¾ à¤¬à¥ˆà¤‚à¤• à¤–à¤¾à¤¤à¤¾ à¤¨à¤¿à¤²à¤‚à¤¬à¤¿à¤¤ à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆ",
        "Su cuenta bancaria ha sido suspendida",
        "Votre compte bancaire a Ã©tÃ© suspendu",
        "Ihr Bankkonto wurde gesperrt",
        "æ‚¨çš„é“¶è¡Œè´¦æˆ·å·²è¢«æš‚åœ",
        "ØªÙ… ØªØ¹Ù„ÙŠÙ‚ Ø­Ø³Ø§Ø¨Ùƒ Ø§Ù„Ù…ØµØ±ÙÙŠ",
        "ã‚ãªãŸã®éŠ€è¡Œå£åº§ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ",
        "ê·€í•˜ì˜ ì€í–‰ ê³„ì¢Œê°€ ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
        "Ğ’Ğ°Ñˆ Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½",
    ]
    
    print("\nğŸ” Testing Language Detection:")
    for message in test_messages:
        detected_lang = detector.detect_language(message)
        print(f"'{message[:30]}...' -> {detector.language_names.get(detected_lang, detected_lang)}")
    
    # Prepare training data (using synthetic data for demonstration)
    print("\nğŸ“š Preparing Training Data:")
    language_data = detector.prepare_multilingual_data("data/sms_dataset.csv")
    
    # Train models
    print("\nğŸ¤– Training Models:")
    detector.train_multilingual_models(language_data)
    
    # Test classification
    print("\nğŸ§ª Testing Multilingual Classification:")
    test_cases = [
        ("Your account has been suspended. Click here to verify", "en"),
        ("à¤†à¤ªà¤•à¤¾ à¤¬à¥ˆà¤‚à¤• à¤–à¤¾à¤¤à¤¾ à¤¨à¤¿à¤²à¤‚à¤¬à¤¿à¤¤ à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤•à¥à¤²à¤¿à¤• à¤•à¤°à¥‡à¤‚", "hi"),
        ("Su cuenta bancaria ha sido suspendida. Haga clic", "es"),
        ("Votre compte bancaire a Ã©tÃ© suspendu. Cliquez", "fr"),
        ("Ihr Bankkonto wurde gesperrt. Klicken Sie", "de"),
        ("æ‚¨çš„é“¶è¡Œè´¦æˆ·å·²è¢«æš‚åœã€‚ç‚¹å‡»é‡æ–°æ¿€æ´»", "zh"),
        ("ØªÙ… ØªØ¹Ù„ÙŠÙ‚ Ø­Ø³Ø§Ø¨Ùƒ Ø§Ù„Ù…ØµØ±ÙÙŠ. Ø§Ù†Ù‚Ø± Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙØ¹ÙŠÙ„", "ar"),
        ("ã‚ãªãŸã®éŠ€è¡Œå£åº§ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸã€‚ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„", "ja"),
        ("ê·€í•˜ì˜ ì€í–‰ ê³„ì¢Œê°€ ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í´ë¦­í•˜ì„¸ìš”", "ko"),
        ("Ğ’Ğ°Ñˆ Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑÑ‡ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½. ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ", "ru"),
    ]
    
    for message, expected_lang in test_cases:
        result = detector.classify_message(message)
        print(f"\n{detector.language_names[expected_lang]}:")
        print(f"  Message: {message[:50]}...")
        print(f"  Detected Language: {result['language_name']}")
        print(f"  Prediction: {result['prediction']}")
        print(f"  Confidence: {result['confidence']:.3f}")
    
    # Save models
    print("\nğŸ’¾ Saving Models:")
    detector.save_models()
    
    print("\nâœ… Multilingual training completed!")

if __name__ == "__main__":
    main() 